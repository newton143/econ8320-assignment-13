{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Modeling in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Customization vs Rapid Development\n",
    "\n",
    "As we know from (painful?) experience, Python is powerful because of its ability to leverage `numpy` and `scipy` to implement any statistical model from scratch. We can write the requisite matrix algebra, or the relevant likelihood function, and from there can optimize our model, calculate confidence intervals, and report the output of that model through data frames, lists, or printed tables. Building our own models is great! We get to build a model based on the exact context and assumptions of our problem, and therefore get exactly the model that we wanted. Unfortunately, it takes a LOT of time!\n",
    "\n",
    "This lesson will provide our first exposure to pre-written statistical modeling in Python. We will be able to use only a couple of lines of code to implement complex and valuable statistical and machine learning models. Because the most costly asset in programming is the time that we spend debugging and writing code (running code is MUCH faster and cheaper than the time spent writing code), we are always looking for ways to avoid writing code that someone else has already written.\n",
    "\n",
    "`statsmodels` is a library that covers the majority of regression models commonly used by economists and statisticians in other fields.\n",
    "\n",
    "`sklearn` is an analogous library that covers machine learning models (aside from deep neural networks, which have their own implementations).\n",
    "\n",
    "Each of these libraries is highly optimized to provide performant implementations of models that we use regularly, and allow us to avoid writing these models from scratch unless we need to customize our model for some specific use case! This is great news! You'll never have to think about writing your own linear or logistic regression from scratch again!\n",
    "\n",
    "Let's dive in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`statsmodels` makes statistics in Python easy! The library contains tools for regressions ranging from linear regression, to logistic regression, count regressions (negative binomial and poisson), various options for robust covariance measures, and tools to implement time series models as well! There are also really useful tools for assisting in creating our regression model based on any structure that best suits us.\n",
    "\n",
    "We can import `statsmodels` in one of two ways:\n",
    "\n",
    "1) With support for R-style formulas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    /opt/conda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
    "      import pandas.util.testing as tm\n",
    "\n",
    "\n",
    "This is probably the best way to import our data if we are doing regression analysis for causal inference. In these cases, we are not typically trying to make predictions as new data arrives, and so we do not need to have tools ready to analyze new data using our existing regression models.\n",
    "\n",
    "2) Import `statsmodels` to use pre-built numpy arrays as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have other tools that we can use, but we need to manually arrange our `x` and `y` matrices. It looks clunky at first, but can be useful when we are building predictive pipelines using regression models, or when we might want to use both `statsmodels` and `sklearn` with the same data source.\n",
    "\n",
    "Let's start with option 1..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using formulas, we prepare our dataset by importing the data into a Pandas `DataFrame`. We should take care that each of our variables has a name with \n",
    "1) **No spaces**\n",
    "2) No symbols\n",
    "3) Made up of letters and numbers (also can't have a number as the first character)\n",
    "\n",
    "Our code so far might look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8320/blob/master/AssignmentData/assignment8Data.csv?raw=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>year</th>\n",
       "      <th>datanum</th>\n",
       "      <th>serial</th>\n",
       "      <th>hhwt</th>\n",
       "      <th>cpi99</th>\n",
       "      <th>region</th>\n",
       "      <th>statefip</th>\n",
       "      <th>countyfips</th>\n",
       "      <th>...</th>\n",
       "      <th>ind5</th>\n",
       "      <th>ind6</th>\n",
       "      <th>ind7</th>\n",
       "      <th>ind8</th>\n",
       "      <th>ind9</th>\n",
       "      <th>ind10</th>\n",
       "      <th>ind11</th>\n",
       "      <th>weekswork</th>\n",
       "      <th>totalhrs</th>\n",
       "      <th>hrwage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20134147</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>730568.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.704</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2080.0</td>\n",
       "      <td>9.134615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20134148</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>730568.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.704</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2080.0</td>\n",
       "      <td>12.019231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20134149</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>730569.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.704</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20134151</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>730571.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.704</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2080.0</td>\n",
       "      <td>19.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20134152</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>730571.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.704</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2080.0</td>\n",
       "      <td>15.384615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     index  year  datanum    serial   hhwt  cpi99  region  \\\n",
       "0           0  20134147  2014        1  730568.0   23.0  0.704      22   \n",
       "1           1  20134148  2014        1  730568.0   23.0  0.704      22   \n",
       "2           2  20134149  2014        1  730569.0   43.0  0.704      22   \n",
       "3           3  20134151  2014        1  730571.0  110.0  0.704      22   \n",
       "4           4  20134152  2014        1  730571.0  110.0  0.704      22   \n",
       "\n",
       "   statefip  countyfips  ...  ind5  ind6  ind7  ind8  ind9  ind10  ind11  \\\n",
       "0        31           0  ...     0     0     1     0     0      0      0   \n",
       "1        31           0  ...     0     0     0     0     0      0      0   \n",
       "2        31           0  ...     0     0     0     0     0      0      0   \n",
       "3        31           0  ...     0     0     1     0     0      0      0   \n",
       "4        31           0  ...     0     0     0     0     0      0      0   \n",
       "\n",
       "   weekswork  totalhrs     hrwage  \n",
       "0       52.0    2080.0   9.134615  \n",
       "1       52.0    2080.0  12.019231  \n",
       "2        NaN       NaN        NaN  \n",
       "3       52.0    2080.0  19.230769  \n",
       "4       52.0    2080.0  15.384615  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13712 entries, 0 to 13711\n",
      "Data columns (total 95 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  13712 non-null  int64  \n",
      " 1   index       13712 non-null  int64  \n",
      " 2   year        13712 non-null  int64  \n",
      " 3   datanum     13712 non-null  int64  \n",
      " 4   serial      13712 non-null  float64\n",
      " 5   hhwt        13712 non-null  float64\n",
      " 6   cpi99       13712 non-null  float64\n",
      " 7   region      13712 non-null  int64  \n",
      " 8   statefip    13712 non-null  int64  \n",
      " 9   countyfips  13712 non-null  int64  \n",
      " 10  city        13712 non-null  int64  \n",
      " 11  gq          13712 non-null  int64  \n",
      " 12  farm        13712 non-null  int64  \n",
      " 13  hhincome    13712 non-null  int64  \n",
      " 14  multgen     13712 non-null  int64  \n",
      " 15  multgend    13712 non-null  int64  \n",
      " 16  pernum      13712 non-null  int64  \n",
      " 17  perwt       13712 non-null  float64\n",
      " 18  nchild      13712 non-null  int64  \n",
      " 19  nchlt5      13712 non-null  int64  \n",
      " 20  relate      13712 non-null  int64  \n",
      " 21  related     13712 non-null  int64  \n",
      " 22  sex         13712 non-null  int64  \n",
      " 23  age         13712 non-null  int64  \n",
      " 24  marst       13712 non-null  int64  \n",
      " 25  race        13712 non-null  int64  \n",
      " 26  raced       13712 non-null  int64  \n",
      " 27  hispan      13712 non-null  int64  \n",
      " 28  hispand     13712 non-null  int64  \n",
      " 29  school      13712 non-null  int64  \n",
      " 30  educ        13712 non-null  int64  \n",
      " 31  educd       13712 non-null  int64  \n",
      " 32  empstat     13712 non-null  int64  \n",
      " 33  empstatd    13712 non-null  int64  \n",
      " 34  labforce    13712 non-null  int64  \n",
      " 35  occ1990     13712 non-null  int64  \n",
      " 36  ind1990     13712 non-null  int64  \n",
      " 37  wkswork2    13712 non-null  int64  \n",
      " 38  uhrswork    13712 non-null  int64  \n",
      " 39  incwage     13712 non-null  int64  \n",
      " 40  incbus      0 non-null      float64\n",
      " 41  incfarm     0 non-null      float64\n",
      " 42  disabwrk    0 non-null      float64\n",
      " 43  tranwork    13712 non-null  int64  \n",
      " 44  wah         13712 non-null  int64  \n",
      " 45  hsless      13712 non-null  int64  \n",
      " 46  somecoll    13712 non-null  int64  \n",
      " 47  colldegree  13712 non-null  int64  \n",
      " 48  gradschool  13712 non-null  int64  \n",
      " 49  student     13712 non-null  int64  \n",
      " 50  married     13712 non-null  int64  \n",
      " 51  female      13712 non-null  int64  \n",
      " 52  headhouse   13712 non-null  int64  \n",
      " 53  multigen    13712 non-null  int64  \n",
      " 54  farmer      13712 non-null  int64  \n",
      " 55  busowner    13712 non-null  int64  \n",
      " 56  white       13712 non-null  int64  \n",
      " 57  black       13712 non-null  int64  \n",
      " 58  other       13712 non-null  int64  \n",
      " 59  hispanic    13712 non-null  int64  \n",
      " 60  pubemp      13712 non-null  int64  \n",
      " 61  occ1        13712 non-null  int64  \n",
      " 62  occ2        13712 non-null  int64  \n",
      " 63  occ3        13712 non-null  int64  \n",
      " 64  occ4        13712 non-null  int64  \n",
      " 65  occ5        13712 non-null  int64  \n",
      " 66  occ6        13712 non-null  int64  \n",
      " 67  occ7        13712 non-null  int64  \n",
      " 68  occ8        13712 non-null  int64  \n",
      " 69  occ9        13712 non-null  int64  \n",
      " 70  occ10       13712 non-null  int64  \n",
      " 71  occ11       13712 non-null  int64  \n",
      " 72  occ12       13712 non-null  int64  \n",
      " 73  occ13       13712 non-null  int64  \n",
      " 74  occ14       13712 non-null  int64  \n",
      " 75  occ15       13712 non-null  int64  \n",
      " 76  occ16       13712 non-null  int64  \n",
      " 77  occ17       13712 non-null  int64  \n",
      " 78  occ18       13712 non-null  int64  \n",
      " 79  occ19       13712 non-null  int64  \n",
      " 80  occ20       13712 non-null  int64  \n",
      " 81  ind1        13712 non-null  int64  \n",
      " 82  ind2        13712 non-null  int64  \n",
      " 83  ind3        13712 non-null  int64  \n",
      " 84  ind4        13712 non-null  int64  \n",
      " 85  ind5        13712 non-null  int64  \n",
      " 86  ind6        13712 non-null  int64  \n",
      " 87  ind7        13712 non-null  int64  \n",
      " 88  ind8        13712 non-null  int64  \n",
      " 89  ind9        13712 non-null  int64  \n",
      " 90  ind10       13712 non-null  int64  \n",
      " 91  ind11       13712 non-null  int64  \n",
      " 92  weekswork   10008 non-null  float64\n",
      " 93  totalhrs    10008 non-null  float64\n",
      " 94  hrwage      10008 non-null  float64\n",
      "dtypes: float64(10), int64(85)\n",
      "memory usage: 9.9 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that our data set has already been cleaned. If our data has not yet been cleaned, then we need to clean our data prior to working with either `statsmodels` or `sklearn`. This is because regression AND machine learning models require that all information be provided in numeric format. We need to transform text-based data into categorical data (using either ordered numeric columns or binary variable columns generated from our categories), and ensure that all data is represented in the way that we want to use it within our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Equations\n",
    "\n",
    "`statsmodels` incorporates `R`-style regression equations by using the `patsy` library behind the scenes. We will talk more about `patsy` soon. The pattern for regression equations is as follows:\n",
    "\n",
    "```\"dependent variable ~ independent variable + another independent variable + any other independent variables\"```\n",
    "\n",
    "The regression equation will be stored in a string (unlike in `R`), and we put our dependent variable (also called the endogenous variable, or outcome of interest) in the leftmost position within the string. We separate the dependent variable from all independent (exogenous or explanatory) variables using the `~` symbol. Then, each independent variable is separated from the others using `+` operators. \n",
    "\n",
    "The reason is is so important that our column names be properly cleaned before implementing regression analysis is that spaces and other problematic formats for column names will cause problems with our regression equations.\n",
    "\n",
    "### Implementing a Model\n",
    "\n",
    "The first model we might try is a simple linear regression. These are the most common regression models, and typically what someone is referring to when they discuss \"running a regression\". The code is wonderfully simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               hhincome   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                  0.000\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Sun, 17 Nov 2024   Prob (F-statistic):                nan\n",
      "Time:                        15:44:42   Log-Likelihood:            -1.7131e+05\n",
      "No. Observations:               13712   AIC:                         3.426e+05\n",
      "Df Residuals:                   13711   BIC:                         3.426e+05\n",
      "Df Model:                           0                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.0188      0.000    138.414      0.000       0.019       0.019\n",
      "year          37.8564      0.274    138.414      0.000      37.320      38.392\n",
      "==============================================================================\n",
      "Omnibus:                     9819.620   Durbin-Watson:                   1.027\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           250725.793\n",
      "Skew:                           3.151   Prob(JB):                         0.00\n",
      "Kurtosis:                      22.978   Cond. No.                     9.31e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 6.41e-26. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "reg = smf.ols(\"hhincome ~ year\", data=data).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:               hhincome   R-squared:                      -0.000\n",
    "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
    "    Method:                 Least Squares   F-statistic:                      -inf\n",
    "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
    "    Time:                        15:24:28   Log-Likelihood:            -1.7131e+05\n",
    "    No. Observations:               13712   AIC:                         3.426e+05\n",
    "    Df Residuals:                   13711   BIC:                         3.426e+05\n",
    "    Df Model:                           0                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept      0.0188      0.000    138.414      0.000       0.019       0.019\n",
    "    year          37.8564      0.274    138.414      0.000      37.320      38.392\n",
    "    ==============================================================================\n",
    "    Omnibus:                     9819.620   Durbin-Watson:                   1.027\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):           250725.793\n",
    "    Skew:                           3.151   Prob(JB):                         0.00\n",
    "    Kurtosis:                      22.978   Cond. No.                     9.31e+17\n",
    "    ==============================================================================\n",
    "    \n",
    "    Warnings:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "    [2] The smallest eigenvalue is 6.41e-26. This might indicate that there are\n",
    "    strong multicollinearity problems or that the design matrix is singular.\n",
    "\n",
    "\n",
    "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
    "      return self.ess/self.df_model\n",
    "\n",
    "\n",
    "When we run these two lines of code, we are creating, fitting, and reporting on a regression model! It's fast, it's clean, and it's really easy to implement! `sm.ols` is the OLS class of regression models, and takes two required arguments: a regression equation (passed as a string), and a data source (expected to be a `pandas.DataFrame` object). We use the `.fit()` method to complete all of the math that actually solves our regression model. When we call `.summary()` on a fitted regression, we get a printout of the regression summary tables for the model, complete with diagnostic measures, estimates of our beta coefficients, and confidence intervals!\n",
    "\n",
    "If the model is satisfactory, then we are done! (It really is that simple!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I want to keep iterating on my model, I might want to try regressing year on the logged average household incomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       np.log(hhincome)   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                  0.000\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Sun, 17 Nov 2024   Prob (F-statistic):                nan\n",
      "Time:                        15:44:48   Log-Likelihood:                -17363.\n",
      "No. Observations:               13653   AIC:                         3.473e+04\n",
      "Df Residuals:                   13652   BIC:                         3.474e+04\n",
      "Df Model:                           0                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
      "year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
      "==============================================================================\n",
      "Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
      "Skew:                          -1.469   Prob(JB):                         0.00\n",
      "Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "reg = smf.ols(\"np.log(hhincome) ~ year\", data=data[data['hhincome']>0]).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
    "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
    "    Method:                 Least Squares   F-statistic:                      -inf\n",
    "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
    "    Time:                        15:31:18   Log-Likelihood:                -17363.\n",
    "    No. Observations:               13653   AIC:                         3.473e+04\n",
    "    Df Residuals:                   13652   BIC:                         3.474e+04\n",
    "    Df Model:                           0                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
    "    year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
    "    ==============================================================================\n",
    "    Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
    "    Skew:                          -1.469   Prob(JB):                         0.00\n",
    "    Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
    "    ==============================================================================\n",
    "    \n",
    "    Warnings:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "    [2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
    "    strong multicollinearity problems or that the design matrix is singular.\n",
    "\n",
    "\n",
    "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
    "      return self.ess/self.df_model\n",
    "\n",
    "\n",
    "As you can see from the code above, everything is the same, except that we were able to transform household income using `np.log` on the go! We don't even need to create a new column! We can just do it inside of our regression model! We also subset our data so that the log operator doesn't break our model by introducing $-\\infty$ as a possible `hhincome` value.\n",
    "\n",
    "In other cases, it might be useful to create state-level fixed effects by including dummy variables for the states in our `statefip` column. Note that this won't work with our current data, since we only have one state in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       np.log(hhincome)   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                  0.000\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Sun, 17 Nov 2024   Prob (F-statistic):                nan\n",
      "Time:                        15:45:03   Log-Likelihood:                -17363.\n",
      "No. Observations:               13653   AIC:                         3.473e+04\n",
      "Df Residuals:                   13652   BIC:                         3.474e+04\n",
      "Df Model:                           0                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
      "year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
      "==============================================================================\n",
      "Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
      "Skew:                          -1.469   Prob(JB):                         0.00\n",
      "Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data[data['hhincome']>0]).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
    "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
    "    Method:                 Least Squares   F-statistic:                      -inf\n",
    "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
    "    Time:                        15:32:19   Log-Likelihood:                -17363.\n",
    "    No. Observations:               13653   AIC:                         3.473e+04\n",
    "    Df Residuals:                   13652   BIC:                         3.474e+04\n",
    "    Df Model:                           0                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
    "    year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
    "    ==============================================================================\n",
    "    Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
    "    Skew:                          -1.469   Prob(JB):                         0.00\n",
    "    Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
    "    ==============================================================================\n",
    "    \n",
    "    Warnings:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "    [2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
    "    strong multicollinearity problems or that the design matrix is singular.\n",
    "\n",
    "\n",
    "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
    "      return self.ess/self.df_model\n",
    "\n",
    "\n",
    "The `C()` command indicates that we would like to consider the `statefip` variable as a **C**ategorical variable, not a numeric variable. We can transform ANY column using the categorical operator. It is most useful when a column is text-based, or when a column is numeric but should not be treated as a count, ordinal, or continuous variable. We CAN use it on our dependent variable, but this will (unless our dependent variable was binary text data) break our regression model, which expects only a single dependent variable, rather than an array of dependent variables.\n",
    "\n",
    "Sometimes we want to include transformed variables in our model without creating a new column. The `I()` operator allows us to do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Square a variable using the I() function for\n",
    "#   mathematical transformations\n",
    "reg = smf.ols(\"np.log(hhincome) ~ age + I(age**2)\", data=data).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we transform `age` by squaring it (maybe in preparation to create an age-earnings profile?). One line, simple syntax, what could be better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Combine variables using the I() function for\n",
    "#   mathematical transformations\n",
    "reg = smf.ols(\"np.log(hhincome) ~ I(age-educ-5)\", data=data).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example combines TWO columns to create a new measure (proxying experience by subtracting education from age, and subtracting an additional 5 years). All we have to do is describe the relationship that we want to model as an explanatory variable, and we are off to the races! Most operators are fair game, and we can include an arbitrary number of columns in our measure calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More robust modeling\n",
    "\n",
    "If we want to utilize robust standard errors, we can easily update our regression results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:1698: RuntimeWarning: invalid value encountered in subtract\n",
      "  return self.model.wendog - self.model.predict(\n",
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:1733: RuntimeWarning: invalid value encountered in subtract\n",
      "  return np.sum(weights * (model.endog - mean)**2)\n",
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:949: RuntimeWarning: invalid value encountered in subtract\n",
      "  resid = self.endog - np.dot(self.exog, params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       np.log(hhincome)   R-squared:                         nan\n",
      "Model:                            OLS   Adj. R-squared:                    nan\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Sun, 17 Nov 2024   Prob (F-statistic):                nan\n",
      "Time:                        15:52:45   Log-Likelihood:                    nan\n",
      "No. Observations:               13702   AIC:                               nan\n",
      "Df Residuals:                   13701   BIC:                               nan\n",
      "Df Model:                           0                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept        -inf        nan        nan        nan         nan         nan\n",
      "year             -inf        nan        nan        nan         nan         nan\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                     nan\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                  nan\n",
      "Skew:                             nan   Prob(JB):                          nan\n",
      "Kurtosis:                         nan   Cond. No.                     1.66e+18\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.02e-26. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1452: RuntimeWarning: invalid value encountered in subtract\n",
      "  a = op(a[slice1], a[slice2])\n"
     ]
    }
   ],
   "source": [
    "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data).fit()\n",
    "# Use White's (1980) Standard Error\n",
    "reg.get_robustcov_results(cov_type='HC0')\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if we want to cluster our standard errors by state,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:1698: RuntimeWarning: invalid value encountered in subtract\n",
      "  return self.model.wendog - self.model.predict(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The weights and list don't have the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m reg \u001b[38;5;241m=\u001b[39m smf\u001b[38;5;241m.\u001b[39mols(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.log(hhincome) ~ year + C(statefip)\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39mdata)\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Use Cluster-robust Standard Errors\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_robustcov_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcluster\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstatefip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Need to specify groups\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(reg\u001b[38;5;241m.\u001b[39msummary())\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:2601\u001b[0m, in \u001b[0;36mRegressionResults.get_robustcov_results\u001b[1;34m(self, cov_type, use_t, **kwargs)\u001b[0m\n\u001b[0;32m   2597\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adjust_df:\n\u001b[0;32m   2598\u001b[0m         \u001b[38;5;66;03m# need to find number of groups\u001b[39;00m\n\u001b[0;32m   2599\u001b[0m         \u001b[38;5;66;03m# duplicate work\u001b[39;00m\n\u001b[0;32m   2600\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_groups \u001b[38;5;241m=\u001b[39m n_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(groups))\n\u001b[1;32m-> 2601\u001b[0m     res\u001b[38;5;241m.\u001b[39mcov_params_default \u001b[38;5;241m=\u001b[39m \u001b[43msw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcov_cluster\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_correction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_correction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m groups\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   2605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(groups, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\stats\\sandwich_covariance.py:530\u001b[0m, in \u001b[0;36mcov_cluster\u001b[1;34m(results, group, use_correction)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    528\u001b[0m     clusters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(group)\n\u001b[1;32m--> 530\u001b[0m scale \u001b[38;5;241m=\u001b[39m \u001b[43mS_crosssection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m nobs, k_params \u001b[38;5;241m=\u001b[39m xu\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    533\u001b[0m n_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(clusters) \u001b[38;5;66;03m#replace with stored group attributes if available\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\stats\\sandwich_covariance.py:484\u001b[0m, in \u001b[0;36mS_crosssection\u001b[1;34m(x, group)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mS_crosssection\u001b[39m(x, group):\n\u001b[0;32m    476\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''inner covariance matrix for White on group sums sandwich\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \n\u001b[0;32m    478\u001b[0m \u001b[38;5;124;03m    I guess for a single categorical group only,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    482\u001b[0m \n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m     x_group_sums \u001b[38;5;241m=\u001b[39m \u001b[43mgroup_sums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m#TODO: why transposed\u001b[39;00m\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m S_white_simple(x_group_sums)\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tools\\grouputils.py:107\u001b[0m, in \u001b[0;36mgroup_sums\u001b[1;34m(x, group, use_bincount)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(group) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    103\u001b[0m         group \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mfactorize(group)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m    106\u001b[0m         [\n\u001b[1;32m--> 107\u001b[0m             \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbincount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    109\u001b[0m         ]\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(group)\n",
      "\u001b[1;31mValueError\u001b[0m: The weights and list don't have the same length."
     ]
    }
   ],
   "source": [
    "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data).fit()\n",
    "# Use Cluster-robust Standard Errors\n",
    "reg.get_robustcov_results(cov_type='cluster', groups=data['statefip']) # Need to specify groups\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to stick to just `HC0` and cluster-robust standard errors. Below are some of the [covariance options](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.get_robustcov_results.html) that we have:\n",
    "1) `HC0`: White's (1980) Heteroskedasticity robust standard errors\n",
    "2) `HC1`, `HC2`, `HC3`: MacKinnon and White's (1985) alternative robust standard errors, with `HC3` being designed for improved performance in small samples\n",
    "3) `cluster`: Cluster robust standard errors\n",
    "4) `hac-panel`: Panel robust standard errors\n",
    "\n",
    "We should choose the standard errors that best fit our specific data needs, and it is important to realize that this choice is highly context-dependent. The structure and nature of our data should be carefully considered, as should the specific regression model that we are trying to implement.\n",
    "\n",
    "### Time Series Models\n",
    "\n",
    "Not only can we model linear regression, we also have multiple time series options available. We won't go into much detail, since each of these models deserve to have significant time devoted to them, and we just don't have the time in this class.\n",
    "\n",
    "- [ARIMA](http://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html) models\n",
    "- [VAR](http://www.statsmodels.org/dev/generated/statsmodels.tsa.vector_ar.var_model.VAR.html) models\n",
    "- [Exponential Smoothing](https://www.statsmodels.org/stable/tsa.html#exponential-smoothing) models\n",
    "\n",
    "We can run an ARIMA, for example, using code like the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "\nstatsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA have\nbeen removed in favor of statsmodels.tsa.arima.model.ARIMA (note the .\nbetween arima and model) and statsmodels.tsa.SARIMAX.\n\nstatsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and\nis both well tested and maintained. It also offers alternative specialized\nparameter estimators.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatefip\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m31\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhhincome\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      6\u001b[0m y\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mto_datetime(y\u001b[38;5;241m.\u001b[39myear)\n\u001b[1;32m----> 7\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[43mARIMA\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhhincome\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(reg\u001b[38;5;241m.\u001b[39msummary())\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\arima_model.py:45\u001b[0m, in \u001b[0;36mARIMA.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\arima_model.py:29\u001b[0m, in \u001b[0;36mARMA.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(ARIMA_DEPRECATION_ERROR)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: \nstatsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA have\nbeen removed in favor of statsmodels.tsa.arima.model.ARIMA (note the .\nbetween arima and model) and statsmodels.tsa.SARIMAX.\n\nstatsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and\nis both well tested and maintained. It also offers alternative specialized\nparameter estimators.\n"
     ]
    }
   ],
   "source": [
    "# This won't work unless we have multiple years of data (which we currently don't)\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "y = data.loc[data['statefip']==31, ['hhincome','year']]\n",
    "y.index=pd.to_datetime(y.year)\n",
    "reg = ARIMA(y['hhincome'], order=(1,1,0)).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Discrete Outcomes\n",
    "\n",
    "If we have a [binary dependent variable](https://www.statsmodels.org/devel/discretemod.html), we are able to use either [Logit](https://www.statsmodels.org/devel/generated/statsmodels.discrete.discrete_model.Logit.html#statsmodels.discrete.discrete_model.Logit) or [Probit](https://www.statsmodels.org/devel/generated/statsmodels.discrete.discrete_model.Probit.html#statsmodels.discrete.discrete_model.Probit) models to estimate the effect of exogenous variables on our outcome of interest. To fit a Logit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.615837\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "myformula=\"married ~ hhincome + C(statefip) + C(year) + educ\"\n",
    "model= sm.Logit.from_formula(myformula, data=data).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Count Data\n",
    "\n",
    "When modeling count data, we have options such as [Poisson](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Poisson.html#statsmodels.discrete.discrete_model.Poisson) and [Negative Binomial](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.NegativeBinomial.html#statsmodels.discrete.discrete_model.NegativeBinomial) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "PatsyError",
     "evalue": "Error evaluating factor: NameError: name 'educ' is not defined\n    nchild ~ hhincome + C(statefip) + C(year) + educ + married\n                                                ^^^^",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\patsy\\compat.py:36\u001b[0m, in \u001b[0;36mcall_and_wrap_exc\u001b[1;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\patsy\\eval.py:169\u001b[0m, in \u001b[0;36mEvalEnvironment.eval\u001b[1;34m(self, expr, source_name, inner_namespace)\u001b[0m\n\u001b[0;32m    168\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcompile\u001b[39m(expr, source_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVarLookupDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minner_namespace\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_namespaces\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'educ' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPatsyError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/dustywhite7/Econ8310/raw/master/DataSets/auto-mpg.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m myformula\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnchild ~ hhincome + C(statefip) + C(year) + educ + married\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPoisson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_formula\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit()\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:203\u001b[0m, in \u001b[0;36mModel.from_formula\u001b[1;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# with patsy it's drop or raise. let's raise.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 203\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_formula_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m ((endog, exog), missing_idx, design_info) \u001b[38;5;241m=\u001b[39m tmp\n\u001b[0;32m    206\u001b[0m max_endog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_formula_max_endog\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\formula\\formulatools.py:63\u001b[0m, in \u001b[0;36mhandle_formula_data\u001b[1;34m(Y, X, formula, depth, missing)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_util\u001b[38;5;241m.\u001b[39m_is_using_pandas(Y, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 63\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mdmatrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataframe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m         result \u001b[38;5;241m=\u001b[39m dmatrices(formula, Y, depth, return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataframe\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     67\u001b[0m                            NA_action\u001b[38;5;241m=\u001b[39mna_action)\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\patsy\\highlevel.py:309\u001b[0m, in \u001b[0;36mdmatrices\u001b[1;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct two design matrices given a formula_like and data.\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03mThis function is identical to :func:`dmatrix`, except that it requires\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03mSee :func:`dmatrix` for details.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    308\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m EvalEnvironment\u001b[38;5;241m.\u001b[39mcapture(eval_env, reference\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 309\u001b[0m (lhs, rhs) \u001b[38;5;241m=\u001b[39m \u001b[43m_do_highlevel_design\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformula_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lhs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PatsyError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel is missing required outcome variables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\patsy\\highlevel.py:164\u001b[0m, in \u001b[0;36m_do_highlevel_design\u001b[1;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_iter_maker\u001b[39m():\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([data])\n\u001b[1;32m--> 164\u001b[0m design_infos \u001b[38;5;241m=\u001b[39m \u001b[43m_try_incr_builders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformula_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m design_infos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m build_design_matrices(design_infos, data,\n\u001b[0;32m    168\u001b[0m                                  NA_action\u001b[38;5;241m=\u001b[39mNA_action,\n\u001b[0;32m    169\u001b[0m                                  return_type\u001b[38;5;241m=\u001b[39mreturn_type)\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\patsy\\highlevel.py:66\u001b[0m, in \u001b[0;36m_try_incr_builders\u001b[1;34m(formula_like, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formula_like, ModelDesc):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_env, EvalEnvironment)\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdesign_matrix_builders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mformula_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlhs_termlist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mformula_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrhs_termlist\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\patsy\\build.py:693\u001b[0m, in \u001b[0;36mdesign_matrix_builders\u001b[1;34m(termlists, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[0;32m    689\u001b[0m factor_states \u001b[38;5;241m=\u001b[39m _factors_memorize(all_factors, data_iter_maker, eval_env)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;66;03m# Now all the factors have working eval methods, so we can evaluate them\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;66;03m# on some data to find out what type of data they return.\u001b[39;00m\n\u001b[0;32m    692\u001b[0m (num_column_counts,\n\u001b[1;32m--> 693\u001b[0m  cat_levels_contrasts) \u001b[38;5;241m=\u001b[39m \u001b[43m_examine_factor_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_factors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfactor_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;66;03m# Now we need the factor infos, which encapsulate the knowledge of\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;66;03m# how to turn any given factor into a chunk of data:\u001b[39;00m\n\u001b[0;32m    699\u001b[0m factor_infos \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\patsy\\build.py:443\u001b[0m, in \u001b[0;36m_examine_factor_types\u001b[1;34m(factors, factor_states, data_iter_maker, NA_action)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_iter_maker():\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(examine_needed):\n\u001b[1;32m--> 443\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mfactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactor_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m factor \u001b[38;5;129;01min\u001b[39;00m cat_sniffers \u001b[38;5;129;01mor\u001b[39;00m guess_categorical(value):\n\u001b[0;32m    445\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m factor \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cat_sniffers:\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\patsy\\eval.py:568\u001b[0m, in \u001b[0;36mEvalFactor.eval\u001b[1;34m(self, memorize_state, data)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m, memorize_state, data):\n\u001b[1;32m--> 568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\patsy\\eval.py:551\u001b[0m, in \u001b[0;36mEvalFactor._eval\u001b[1;34m(self, code, memorize_state, data)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_eval\u001b[39m(\u001b[38;5;28mself\u001b[39m, code, memorize_state, data):\n\u001b[0;32m    550\u001b[0m     inner_namespace \u001b[38;5;241m=\u001b[39m VarLookupDict([data, memorize_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransforms\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_and_wrap_exc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError evaluating factor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m                             \u001b[49m\u001b[43minner_namespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_namespace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\patsy\\compat.py:43\u001b[0m, in \u001b[0;36mcall_and_wrap_exc\u001b[1;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     new_exc \u001b[38;5;241m=\u001b[39m PatsyError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m                          \u001b[38;5;241m%\u001b[39m (msg, e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e),\n\u001b[0;32m     41\u001b[0m                          origin)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Use 'exec' to hide this syntax from the Python 2 parser:\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraise new_exc from e\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# In python 2, we just let the original exception escape -- better\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# than destroying the traceback. But if it's a PatsyError, we can\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# at least set the origin properly.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, PatsyError):\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "\u001b[1;31mPatsyError\u001b[0m: Error evaluating factor: NameError: name 'educ' is not defined\n    nchild ~ hhincome + C(statefip) + C(year) + educ + married\n                                                ^^^^"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8310/raw/master/DataSets/auto-mpg.csv\")\n",
    "\n",
    "myformula=\"nchild ~ hhincome + C(statefip) + C(year) + educ + married\"\n",
    "\n",
    "model= sm.Poisson.from_formula(myformula, data=data).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other regression \"flavors\", and the best way to learn about what is available through `statsmodels` is to [read the docs](https://www.statsmodels.org/stable/user-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `patsy` library\n",
    "\n",
    "We have been using regression equations in `statsmodels` a lot without really discussing what is happening behind the scenes. `statsmodels` relies on a library called `patsy` to parse regression equations and prepare our data for regression analysis. While `statsmodels` does a great job of incorporating the `patsy` library for us, this isn't always the case. In fact, it is a really valuable tool in many other contexts (think machine learning or deep learning). \n",
    "\n",
    "\n",
    "### Why use `patsy`?\n",
    "\n",
    "We don't necessarily have to use `patsy`. We could just select our variables manually. Creating a column of ones to serve as our intercept column is trivial (you of course remember that from the linear regression assignment). `patsy` is a tool for creating a standardized pipeline to deal with data that is stored in identical formats, and aids us in creating reusable or replicable code. Patsy allows us to separate our endogenous and exogenous variables AND to\n",
    "\t- \"Dummy out\" categorical variables\n",
    "\t- Easily transform variables (square, or log transforms, etc.)\n",
    "\t- Use identical transformations on future data\n",
    "    \n",
    "Even better, `patsy` is just as easy to use as regression equations. We just need to learn about the function wrappers that are necessary to create our processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy as pt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8320/blob/master/AssignmentData/assignment8Data.csv?raw=true\")\n",
    "\n",
    "# To create y AND x matrices\n",
    "y, x = pt.dmatrices(\"hhincome ~ year + educ + married + age\", data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get started, we need to import `patsy`, and we typically give it the two-letter abbreviation `pt`. Once we have imported our data, we use the `pt.dmatrices` function. This function takes a regression equation (again, as a string), and a data source. The returned value is a **tuple** of `y` and `x`. We can break that tuple into two values by using the `y, x = ...` syntax, so that we have a `y` array and an `x` array.\n",
    "\n",
    "We don't have to create BOTH `y` and `x` data, though! We can use the `pt.dmatrix` function to just create an `x` matrix. Maybe we already have a dependent variable, and want to try out variations on our explanatory variables to see how each performs. In this case, our regression equation should have no column name to the left of the `~` symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create ONLY an x matrix\n",
    "x = pt.dmatrix(\"~ year + educ + married + age\", data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more note is that these regression equations automatically include an intercept term. If you do NOT want an intercept term (some regression models and most machine learning models don't use them), then you can add `-1` as an exogenous variable in your regression equation, in order to indicate that you want to eliminate the column of ones that make up the intercept column in our matrix of exogenous regressors.\n",
    "\n",
    "### Categorical Variables\n",
    "\n",
    "Again, we have the functions described in the regression section above available to us as we transform our data. We can create categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create y AND x matrices\n",
    "eqn = \"hhincome ~ C(year) + educ + married + age\"\n",
    "y, x = pt.dmatrices(eqn, data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And (again) we can transform variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# To create y AND x matrices\n",
    "eqn = \"I(np.log(hhincome)) ~ C(year) + educ + married + age + I(age**2)\"\n",
    "y, x = pt.dmatrices(eqn, data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use interaction operators. `*` will interact each value of two columns, and also include the original columns in the regression model. `:` will include only the interaction terms, while omitting the original columns. Check out the [explanation of formulas](https://patsy.readthedocs.io/en/latest/formulas.html) for more details.\n",
    "\n",
    "\n",
    "### SUPER IMPORTANT $\\rightarrow$ Same Transformation on New Data!\n",
    "\n",
    "Often, we will want to build a model with observed data that can make predictions about new observations as those observations are recorded. `patsy` provides a simple function to take the structure of one exogenous matrix and generate another identically structured matrix using new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataNew' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# To create a new x matrix based on our previous version\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m xNew \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mbuild_design_matrices([x\u001b[38;5;241m.\u001b[39mdesign_info], \u001b[43mdataNew\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataNew' is not defined"
     ]
    }
   ],
   "source": [
    "# To create a new x matrix based on our previous version\n",
    "xNew = pt.build_design_matrices([x.design_info], dataNew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we can create a new matrix in the SAME SHAPE as our original `x` matrix by using the `build_design_matrices()` function in `patsy`. \n",
    "\n",
    "We pass a list containing the old design matrix information (because we can actually create many matrices simultaneously), as well as the new data from which to construct our new matrix.\n",
    "\n",
    "Why does recreating our `x` array matter? This process ensures that we always have the same number of categories in our categorical variables. A new, smaller subset of data that is freshly observed may not contain observations of every category, in which case an updated patsy matrix would not contain the correct number of columns! We are able to maintain consistency in our model, making our work replicable. Most importantly, this will streamline the use of `statsmodels` and `sklearn` in the same workflow!\n",
    "\n",
    "Speaking of `sklearn`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn`\n",
    "\n",
    "What `statsmodels` does for regression analysis, `sklearn` does for predictive analytics and machine learning. It is a truly fabulous library. `sklearn` is likely the most popular machine learning library, and has a standard API to make using the library VERY simple. Even better, it's documentation is some of the nicest documentation you will find anywhere, and contains incredible detail about how to implement models, as well as lessons about the \"how\" and \"why\" of using each model. You couldn't write a better textbook about machine learning than the documentation for `sklearn`.\n",
    "\n",
    "Below, we will briefly discuss some of the models that are most commonly utilized from `sklearn`. Details will be sparse. We are mostly focused on the code implementation of these models. More detail on how machine learning models work is provided in  Business Forecasting, and is outside the scope of this course.\n",
    "\n",
    "### Decision Tree Classification (and Regression)\n",
    "\n",
    "[Classification](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) and [Regression](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) Trees (CARTs) are the standard jumping-off point for exploring machine learning. They are very easy to implement in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy: 0.9753162225224119\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import patsy as pt\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/roomOccupancy.csv\")\n",
    "\n",
    "y, x = pt.dmatrices(\"Occupancy ~ CO2\", data=data)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(x, y.squeeze())\n",
    "\n",
    "pred = clf.predict(x)\n",
    "\n",
    "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In-sample accuracy: 0.9753162225224119\n",
    "\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "We also implement [Support Vector Machines](http://scikit-learn.org/stable/modules/svm.html#svm) for both [classification](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) and [regression](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy: 0.9184575709198084\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf = clf.fit(x, y.squeeze())\n",
    "\n",
    "pred = clf.predict(x)\n",
    "\n",
    "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    /opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
    "      \"avoid this warning.\", FutureWarning)\n",
    "\n",
    "\n",
    "    In-sample accuracy: 0.9397028122313643\n",
    "\n",
    "\n",
    "Can you see the API pattern yet?\n",
    "\n",
    "### Random Forest Models\n",
    "\n",
    "Again, available in both [classification](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) and [regression](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) flavors, these models are aggregations of many randomized Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy: 0.974702198207049\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf = clf.fit(x, y.squeeze())\n",
    "\n",
    "pred = clf.predict(x)\n",
    "\n",
    "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In-sample accuracy: 0.9748250030701215\n",
    "\n",
    "\n",
    "There MUST be a pattern here...\n",
    "\n",
    "Of course there is! We import our classifier (or regressor), then create an instance of that object. We can name it `clf` or anything else that we prefer. From there, the process is the same:\n",
    "- Use the `.fit()` method, passing in the relevant data for our context\n",
    "- Create predictions using our fitted model with `.predict()` and new exogenous data (or the old data to test in-sample fit)\n",
    "- Measure the performance of our model with `accuracy_score`, or any other metric that can describe performance given a specific use case\n",
    "\n",
    "### More from `sklearn`\n",
    "\n",
    "Many other tools are also available to aid in the data cleaning process through `sklearn`. Some of these are:\n",
    "\n",
    "- [Principal Component Analysis (PCA)](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\n",
    "- [Factor Analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis)\n",
    "- Many [Cross-Validation Algorithms](http://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Hyperparameter Tuning](http://scikit-learn.org/stable/modules/grid_search.html)\n",
    "   - Finding the correct parameters for a decision tree or random forest, for example\n",
    "- [Model Evaluation Tools](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [Plotting decision trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve-it!\n",
    "\n",
    "Using the wage data provided here (https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/wagePanelData.csv), create a linear regression model to explain and/or predict wages. Your data set should be labeled `data` and your fitted model should be stored as `reg`. If you do not name the model correctly, you won't get any points!\n",
    "\n",
    "Please put all your code for this exercise in the cell labeled `#si-linear-regression` file found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4165 entries, 0 to 4164\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   id                     4165 non-null   int64  \n",
      " 1   year                   4165 non-null   int64  \n",
      " 2   years_experience       4165 non-null   int64  \n",
      " 3   weeks_worked           4165 non-null   int64  \n",
      " 4   occupation_code        4165 non-null   int64  \n",
      " 5   industry_code          4165 non-null   int64  \n",
      " 6   south_region           4165 non-null   int64  \n",
      " 7   metropolitan_resident  4165 non-null   int64  \n",
      " 8   ms                     4165 non-null   int64  \n",
      " 9   female                 4165 non-null   int64  \n",
      " 10  union_member           4165 non-null   int64  \n",
      " 11  education              4165 non-null   int64  \n",
      " 12  is_black               4165 non-null   int64  \n",
      " 13  log_wage               4165 non-null   float64\n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 455.7 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>log_wage</td>     <th>  R-squared (uncentered):</th>      <td>   0.975</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.975</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>8.061e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 17 Nov 2024</td> <th>  Prob (F-statistic):</th>           <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:37:03</td>     <th>  Log-Likelihood:    </th>          <td> -6159.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  4165</td>      <th>  AIC:               </th>          <td>1.232e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  4163</td>      <th>  BIC:               </th>          <td>1.234e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education</th>        <td>    0.4116</td> <td>    0.002</td> <td>  181.858</td> <td> 0.000</td> <td>    0.407</td> <td>    0.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>years_experience</th> <td>    0.0610</td> <td>    0.001</td> <td>   46.517</td> <td> 0.000</td> <td>    0.058</td> <td>    0.064</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 7.142</td> <th>  Durbin-Watson:     </th> <td>   0.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.028</td> <th>  Jarque-Bera (JB):  </th> <td>   7.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.101</td> <th>  Prob(JB):          </th> <td>  0.0277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.986</td> <th>  Cond. No.          </th> <td>    3.91</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &    log\\_wage     & \\textbf{  R-squared (uncentered):}      &     0.975   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared (uncentered):} &     0.975   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       }          & 8.061e+04   \\\\\n",
       "\\textbf{Date:}             & Sun, 17 Nov 2024 & \\textbf{  Prob (F-statistic):}          &     0.00    \\\\\n",
       "\\textbf{Time:}             &     16:37:03     & \\textbf{  Log-Likelihood:    }          &   -6159.4   \\\\\n",
       "\\textbf{No. Observations:} &        4165      & \\textbf{  AIC:               }          & 1.232e+04   \\\\\n",
       "\\textbf{Df Residuals:}     &        4163      & \\textbf{  BIC:               }          & 1.234e+04   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     }          &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     }          &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                           & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{education}         &       0.4116  &        0.002     &   181.858  &         0.000        &        0.407    &        0.416     \\\\\n",
       "\\textbf{years\\_experience} &       0.0610  &        0.001     &    46.517  &         0.000        &        0.058    &        0.064     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  7.142 & \\textbf{  Durbin-Watson:     } &    0.296  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.028 & \\textbf{  Jarque-Bera (JB):  } &    7.169  \\\\\n",
       "\\textbf{Skew:}          & -0.101 & \\textbf{  Prob(JB):          } &   0.0277  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.986 & \\textbf{  Cond. No.          } &     3.91  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] RÂ² is computed without centering (uncentered) since the model does not contain a constant. \\newline\n",
       " [2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:               log_wage   R-squared (uncentered):                   0.975\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.975\n",
       "Method:                 Least Squares   F-statistic:                          8.061e+04\n",
       "Date:                Sun, 17 Nov 2024   Prob (F-statistic):                        0.00\n",
       "Time:                        16:37:03   Log-Likelihood:                         -6159.4\n",
       "No. Observations:                4165   AIC:                                  1.232e+04\n",
       "Df Residuals:                    4163   BIC:                                  1.234e+04\n",
       "Df Model:                           2                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "education            0.4116      0.002    181.858      0.000       0.407       0.416\n",
       "years_experience     0.0610      0.001     46.517      0.000       0.058       0.064\n",
       "==============================================================================\n",
       "Omnibus:                        7.142   Durbin-Watson:                   0.296\n",
       "Prob(Omnibus):                  0.028   Jarque-Bera (JB):                7.169\n",
       "Skew:                          -0.101   Prob(JB):                       0.0277\n",
       "Kurtosis:                       2.986   Cond. No.                         3.91\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#si-linear-regression\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from the provided URL\n",
    "url = \"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/wagePanelData.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Check the first few rows to understand the structure of the data\n",
    "data.head()\n",
    "data.info()\n",
    "\n",
    "# Define the independent variables (predictors)\n",
    "X = data[['education', 'years_experience']]  # Use the columns: age, education, years_experience\n",
    "\n",
    "# The dependent variable (log-transformed wage)\n",
    "y = data['log_wage']\n",
    "\n",
    "# Fit the linear regression model\n",
    "reg = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display the regression summary\n",
    "reg.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve-it!\n",
    "\n",
    "Import the pass/fail data for students in Portugal found here(https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/passFailTrain.csv), and create a logistic regression model \n",
    "using `statsmodels` that can estimate the likelihood of students passing or failing class. The dependent variable is contained in the column called `G3`, which takes the value `1` when the student has a passing final grade, and `0` otherwise.\n",
    "\n",
    "Call your fitted model `reg`, and place all code for this exercise in the cell labeled `#si-logistic-regression` file found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.313771\n",
      "         Iterations 23\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                     G3   No. Observations:                  296\n",
      "Model:                          Logit   Df Residuals:                      262\n",
      "Method:                           MLE   Df Model:                           33\n",
      "Date:                Sun, 17 Nov 2024   Pseudo R-squ.:                  0.4980\n",
      "Time:                        16:25:45   Log-Likelihood:                -92.876\n",
      "converged:                       True   LL-Null:                       -185.01\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.314e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -37.8399        nan        nan        nan         nan         nan\n",
      "Unnamed: 0     0.0002      0.005      0.044      0.965      -0.009       0.009\n",
      "school        -0.2226      1.157     -0.192      0.847      -2.491       2.046\n",
      "sex            0.4512        nan        nan        nan         nan         nan\n",
      "age           -0.6683      0.178     -3.751      0.000      -1.018      -0.319\n",
      "address       -0.2726      0.451     -0.604      0.546      -1.157       0.612\n",
      "famsize       -0.0655      0.466     -0.141      0.888      -0.979       0.848\n",
      "Pstatus       -0.6200        nan        nan        nan         nan         nan\n",
      "Medu           0.1185        nan        nan        nan         nan         nan\n",
      "Fedu          -0.1614        nan        nan        nan         nan         nan\n",
      "Mjob          -0.2220      0.238     -0.931      0.352      -0.689       0.245\n",
      "Fjob           0.0388      0.059      0.661      0.509      -0.076       0.154\n",
      "reason         0.1785      0.179      1.000      0.317      -0.171       0.528\n",
      "guardian       0.3266        nan        nan        nan         nan         nan\n",
      "traveltime     0.2017      0.338      0.597      0.550      -0.460       0.863\n",
      "studytime     -0.1899      0.131     -1.452      0.147      -0.446       0.067\n",
      "failures      -0.4699      0.283     -1.660      0.097      -1.025       0.085\n",
      "schoolsup     -0.4917      0.561     -0.877      0.381      -1.591       0.608\n",
      "famsup        -1.0076      0.563     -1.789      0.074      -2.112       0.096\n",
      "paid           0.7327        nan        nan        nan         nan         nan\n",
      "activities    -0.3223      0.505     -0.638      0.524      -1.313       0.668\n",
      "nursery       -0.4218      0.537     -0.785      0.432      -1.475       0.631\n",
      "higher         1.4193      1.467      0.967      0.333      -1.457       4.295\n",
      "internet      -0.0501      0.463     -0.108      0.914      -0.958       0.857\n",
      "romantic      -0.5576      0.439     -1.270      0.204      -1.418       0.303\n",
      "famrel         0.3461      0.322      1.074      0.283      -0.285       0.978\n",
      "freetime      -0.0304      0.178     -0.171      0.865      -0.380       0.319\n",
      "goout         -0.3697      0.505     -0.731      0.465      -1.360       0.621\n",
      "Dalc          -0.2911        nan        nan        nan         nan         nan\n",
      "Walc           0.1672        nan        nan        nan         nan         nan\n",
      "health        -0.0620      0.241     -0.257      0.797      -0.534       0.410\n",
      "absences      -0.0433        nan        nan        nan         nan         nan\n",
      "G1             3.6013      0.916      3.933      0.000       1.807       5.396\n",
      "G2            49.1051        nan        nan        nan         nan         nan\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "#si-logistic-regression\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import requests\n",
    "\n",
    "# Load the data from the provided URL\n",
    "url = \"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/passFailTrain.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Check the first few rows to understand the structure of the data\n",
    "data.head()\n",
    "\n",
    "\n",
    "# Define the dependent variable (G3) and independent variables\n",
    "y = data['G3']  # Dependent variable (final grade, pass/fail)\n",
    "X = data.drop('G3', axis=1)  # Independent variables (all columns except G3)\n",
    "\n",
    "# Add a constant to the independent variables for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "reg = sm.Logit(y, X).fit()\n",
    "\n",
    "# Display the regression summary\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve-it!\n",
    "\n",
    "Use the data on NFL franchise values included in the NFL Valuation data source (https://raw.githubusercontent.com/dustywhite7/Econ8320/master/AssignmentData/assignment12Data.csv) file to implement a Random Forest Classifier in sklearn using 100 trees to predict team-years when `Playoffs` takes the value `1` (when a team made the playoffs in that season).\n",
    "\n",
    "- Use Patsy to create `x2` and `y2` matrices\n",
    "- Create the classifier\n",
    "- Fit the classifier, and store the fitted model with the name `playoffForest`\n",
    "\n",
    "Place all code for this exercise in the cell labeled `#si-random-forest` file found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "#si-random-forest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import patsy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import unittest\n",
    "import re\n",
    "\n",
    "# Load the data\n",
    "url = \"https://raw.githubusercontent.com/dustywhite7/Econ8320/master/AssignmentData/assignment12Data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Use Patsy to create the design matrices\n",
    "y2, x2 = patsy.dmatrices('Playoffs ~ Team + Year + Value + Revenues + TVDeal + SuperBowl', data=data)\n",
    "\n",
    "# Random Forest Classifier\n",
    "playoffForest = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "playoffForest.fit(x2, y2.squeeze())  # Make sure y2 is flattened to a 1D array\n",
    "\n",
    "# Predict on the same data\n",
    "predictions = playoffForest.predict(x2)\n",
    "\n",
    "# Calculate in-sample accuracy\n",
    "accuracy = accuracy_score(y2.squeeze(), predictions)\n",
    "\n",
    "print(\"In-sample accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
